---
title: "Ancianos"
author: "Ferran Garcia"
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warnings = FALSE, message = FALSE, fig.pos = "H")
setwd("C:/Users/ferran/Documents/Universitat/US/GEO/trabajos/ancianos_")
```

```{r}
library(knitr)
library(corrplot)
library(dplyr)
library(readxl)
library(psych)
```

```{r}
source("scripts/import.R")
source("scripts/tabs.R")
```

\newpage
# Introducción

## Contexto

El envejecimiento demográfico emerge como un fenómeno geográfico crítico que podría acarrear serias implicaciones macroeconómicas estructurales en nuestra sociedad. En consecuencia, resulta imperativo abordar de manera inmediata políticas orientadas a mejorar la calidad de vida de la población mayor, fundamentadas en estudios que delineen su situación espacial y social, particularmente en aquellos entornos urbanos donde su presencia es más significativa.

## Objetivos

En el actual escenario, nos enfrentamos al desafío de localizar áreas urbanas en el municipio de Sevilla que compartan una estructura demográfica similar, especialmente en lo que respecta a la población mayor. Para abordar este objetivo, emplearemos distintos métodos de análisis estadístico multivariante y espacial. La meta es desarrollar servicios sociales y de asistencia que tomen en cuenta las características específicas de los ancianos y su distribución geográfica en la ciudad. En este contexto, nuestro enfoque consiste en identificar segmentos homogéneos de la población anciana en áreas urbanas. La ejecución de este proceso nos permitirá diseñar servicios sociales y de asistencia que se adapten a las necesidades particulares de los ancianos, considerando tanto su tipología como su ubicación en la ciudad.

## Estructura

\newpage
# Metodología

Reducción dimensional:

- Selección de variables
  + Correlación
  + Sentido común

- Adecuación de la muestra
  + KMO
  + Barlett: ¿Existe correlación?
  
- Análisis factorial:
  + Número de factores:(Regla de Kaiser, Proporción de varianza explicada, Scree plot)
  + Extracción y rotación de factores (R)
  + Puntuaciones factoriales (Descriptiva)

Clústers:

- Métodos jerárquicos
  + Distancias (distintos tipos)
  + Dirección (agregar o disolver)
  
- Métodos no jerárquicos
  + K-medias

- Métodos fuzzy

## Técnicas

## Herramientas

\newpage
# Reducción dimensional

Disponemos de muchas variables, eso motiva las técnicas de reducción dimensional. Usaremos análisis factorial de componentes pricipales, la idea es sintetizar todas las medidas disponibles en variables latentes. Lo haremos aprovechando la correlación que comparten. Para ello seleccionamos las variables que incluiremos a partir de un breve análisis exploratorio. Luego comprobaremos si es pertinente usar los procedimientos en cuestión en el conjunto de datos resultante. Entonces realizaremos varios ajustes con diferente número de factores hasta alcanzar la descomposición más satisfactoria en términos de representabilidad de las variables iniciales. Finalmente aplicaremos técnicas de rotación con tal de lograr factores ortogonales que sean interpretables. Con éstos extraeremos las puntuaciones factoriales que más adelante usaremos para crear clústers.

## Selección de variables y adecuación de los datos

La selección de variables sirve para subsanar potenciales problemas y afinar en el cálculo de factores. Es importante tanto para que se pueda realizar el análisis factorial como para que éste sea eficiente y se ajuste un modelo robusto con interpretación relativamente simple evitando sobreajuste.

Por ejemplo, si nos fijamos en las variables demográficas veremos que se incluyen múltiples medidas de población quedando algunas determinadas completamente por el resto. En cuanto a la *situación laboral* tenemos población activa, ocupada, inactiva y parados.Entonces el total queda determinado por otra variable, ya sea la población total o u subgrupo de ésta y por tanto son linealmente dependientes. Por tanto la matriz de correlaciones no será inverible y en consecuencia no se podrá realizar el análisis factorial. Lo mismo ocurre con algunas variables del estado de las viviendas. En el annexo se muestra una lista completa de las variables que han sido excluidas por este motivo.

Por otro lado es importante que las variables seleccionadas estén correlacionadas entre sí. Para comprobarlo representamos gráficamente la correlación entre las variables seleccionadas y recogemos en una tabla estadísticos relevantes para la selección de variables como: 

  - Estimación inicial de comunalidades^[Correlación múltiple al cuadrado]
  - Suma de correlaciones absolutas
  - Número de variables no correlacionadas^[Test de correlación de Pearson, nivel de significación del 5%]

Las variables que se muestran en la tabla quedan excluidas del análisis factorial.

```{r}
#| fig.cap = "Se muestran las variables con una comunalidad estimada menor al 70%,
#|  menos de 30 correlaciones no significativas al 5% y una suma de correlaciones absolutas menor a su mediana."
cor_tab0
```

```{r, out.width="100%", fig.align='center'}
#| fig.cap = "Correlaciones entre todas las variables."
include_graphics("grafs/cor/cor_x0.png")
```

Veamos ahora si el conjunto de datos seleccionados resulta apropiado para el análisis factorial o si debemos seleccionar variables con un criterio más estricto. Para ello realizamos varios tests:

**Esfericidad de Barlett:**

- Hipóteis nula: La matriz de correlaciones es la identidad (no existe ninguna correlación)
- Bajo $H_0$ el estadístico de contraste sigue una $\chi^2$, asintóticamente
- Disponemos de más de 400 observaciones, se sostiene la suposición asintótica

```{r, echo = TRUE}
cortest.bartlett(R1, n = nrow(X1))[["p.value"]] # podemos rechazar H_0
```

**Kaiser, Meyer, Olkin:**

- Medida de adecuación de la muestra para un análisis factorial (basa)
- Compara la proporción de correlación y la [correlación parcial](https://en.wikipedia.org/wiki/Kaiser%E2%80%93Meyer%E2%80%93Olkin_test#Measure_of_sampling_adequacy)]
- Está acotada entre 0 y 1, según Kaiser:
    + Valores mayores a 0,9 son maravillosos
    + Valores mayores a 0,8 son meritorios
    + Valores mayores a 0,7 son medios
    + Valores mayores a 0,6 son mediocres
    + Valores mayores a 0,5 son miserables

```{r, echo = TRUE}
KMO(R1)[[1]] # según Kaiser, la adecuación de la muestra es meritoria
```

\newpage
## Número, rotación y puntuaciones factoriales

Ahora nuestra intención es identificar una estructura latente dentro del conjunto de datos. Se focaliza el interés en los factores capaces de explicar una proporción significativa de la variabilidad presente en los datos. Para determinar cuántos factores debemos retener, empleamos dos criterios. En primer lugar, se puede dibujar un scree plot para evaluar los *eigenvalues* de los factores estimados. Además, aplicamos la Regla de Kaiser, que sugiere retener aquellos factores cuyos *eigenvalues* superen la unidad. Otra estrategia consiste en seleccionar tantos factores como sean necesarios para explicar alrededor de un 70% de la variabilidad de los datos.

**Regla de Kaiser:**

En el ámbito del álgebra una matriz (como la de correlaciones) se puede interpretar como una transformación lineal. Los *eigenvectors* asociados a una transformación son aquellos vectores cuya dirección es invariable a la misma y los *eigenvalues* aparejados son la medida en la que la dirección se alarga o mengua en magnitud. Bajo ciertas condiciones de regularidad estos vectores forman una base sobre la que se puede descomponer la transformación en cuestión. La idea de Kaiser es tomar tantos factores como autovectores de "alarguen" (su importancia crezca) en el proceso de descomposición.^[Para más información sobre los *eigenvectors* y *eigenvalues* consultar [este enlace](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)]

```{r, echo = TRUE}
autov_1 = eigen(R1)[["values"]]; sort(autov_1, decreasing = T) %>% round(digits = 2)
sum(autov_1 > 1) # según el criterio de Kaiser, usaremos 8 factores
```
En este caso Kaiser recomienda tomar 8 factores. No obstante los últimos están muy cerca de 1, es algo que debemos tener en cuenta para la selección del número de factores.

**Proporción de la varianza explicada:**

Para emplear el método de la varianza explicada por factores necesitamos realizar un análisis factorial como tal. De manera que ajustamos un modelo co un número arbitrario de factores y comprobamos cuántos son necesarios 

```{r, echo = TRUE}
fit_1 = factanal(na.omit(X1[,-1]), factors = 8, lower = .01)
print(fit_1[["loadings"]], digits = 2, cutoff = .5, sort = TRUE)
```

Esta salida muestra por un lado las cargas factoriales de cada variable inicial y por el otro la proporción de varianza explicada por cada uno de los factores. Nos interesa la parte final. Nos fijamos en que se explica alrededor del 70% de la varianza con 7 u 8 factores. También notamos que los primeros factores recogen sustancialmente más varianza que el resto.

**Scree plot:**

En este caso interpretamos gráficamente los *eigenvalues* asociados a factores con el número que usamos. Notamos que los cuatro primeros son relativamente más altos a los dos siguientes, que resultan ser los últimos que superan el umbral de 1. En este caso el gráfico sugiere usar alrededor de 6 factores.

```{r}
sim_1 = fa.parallel(R1, n.obs = 441, fm = "ml", fa = 'fa')
```

**Interpretación en conjunto:**

Los diversos métodos indican una cantidad de factores similar: Entre 6 y 8. Puesto que utilizar 7 factores concuerda con los resultados de los diferentes criterios y está enmedio de las sugerencias de cada uno usaremos 7 factores para reducir la dimensionalidad de los datos.

```{r, echo = TRUE}
fit_2 = factanal(na.omit(X1[,-1]), factors = 7, lower = .007, scores = "regression")
print(fit_2[["loadings"]], digits = 2, cutoff = .5, sort = TRUE)
```

Representamos los pesos factoriales más importantes (almenos 0,5) para cada variable. Una versión más detallada a la que nos referimos en las siguientes líneas se puede encontrar en ele annexo. Como era de esperar los primeros factores cargan más información que los siguientes y por tanto su interpretación es más compleja. A continuación proponemos una breve e intuitiva interpretación intuitiva sobre cada uno:

**Factor 1: Necesidad futura** 
Contiene información sobre variables demográficas y del hogar. Por un lado indica una población sustancial que en general es de procedencia cercana^[de Sevilla o Andalucía.] y de edad no muy avanzada.^[se relaciona más con los tramos de edad más bajos.] Teniendo en cuenta las variables laborales y educativas está relacionado con una estrato social medio-bajo. En resumen, describe zonas en las que servicios sociales y asistenciales serán necesarios en un futuro cercano.

**Factor 2: Ahora autosuficiente**
En cierta manera es la antítesis del anterior factor. Lo único que comparten es la relación con la cantidad de habitantes. Este factor describe zonas relativamente pobladas por personas con procedencia lejana^[Fuera de Andalucía y en mayor medida de fuera del España.] con capacidad económica presumiblemente alta^[Relacionado con nivel educativo alto y trabajo fijo.] y viviendas con caché.^[Viviendas grandes en términos de espacio y habitaciones.] Cabe destacar que en cierta manera la relación con los tramos de edad describe una población más envejecida, sin embargo la conexión es menor que en el caso anterior.

El resto de factores representan mucho menos la situación que los dos primeros y su nombre explica en sí mismo la interpretación asociada a cada una. A continuación comentamos la descriptiva de las puntuaciones factoriales y en la siguiente sección agruparemos las secciones censales en clústers.

**Factor 3: Faltan suministros básicos**

**Factor 4: Clase media-baja**

**Factor 5: Baja densidad de población**

**Factor 6: Clase media-alta**

**Factor 7: Vivienda colectiva**

```{r}
aux = describe(fit_2$scores)[,-c(1,2, 3, 4, 6, 13)] %>% round(digits = 2)
nb_fact = c(
  "Necesidad futura",
  "Ahora autosuficiente",
  "Faltan suministros básicos",
  "Clase media-baja",
  "Baja densidad de población",
  "Clase media-alta",
  "Vivienda colectiva"
)
rownames(aux) = nb_fact

kable(aux, booktabs = TRUE, format = "latex", align = "c",
      caption = "Estadísticos descriptivos de las puntuaciones factoriales",
        row.names = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = FALSE)
```

Como es habitual las variables han sido estandarizadas antes del análisis, de modo que pasamos por alto la media y varianza. Nos fijamos en el signo de la mediana y la asímetría (skew): Todas las variables consendan una cola derecha pesada, en otras palabras para cada factor existen pocas secciones con puntuaciones muy altas mientras que muchas ligeramente bajas. El caso extremo es el factor de *Vivienda colectiva*, es plausible que las viviendas colectivas se encuentren muy concentradas en secciones particulares.

Destacamos que la mediana más baja es por orden la de los factores *Baja densidad de población*, *Clase media-baja* y *Ahora autosuficiente* intuimos que seguramente habrá algunas secciones con muy densidad muy baja de población, muchas personas de clase medio-baja y muchas personas con poder adquisitivo alto. Recogemos los histogramas correspondientes a todos los factores en el annexo.

\newpage
# Clústers

## Método jerárquico

## Método de k-medias

\newpage
# Annexo

## Variables excluidas

### Variables demográficas
- Categoría de edad: 60-64 años
- Sexo: Población masculina
- Procedencia: Provincial
- Nivel de estudios: Segundo grado
- Situación laboral: Activos
- Situación de actividad: Ocupados
- Categoría profesional: Servicios
- Posición profesional: Empleados
- Temporalidad profesional: Otro
- Residencia: Alojamiento


### Variables de la vivienda
- Tamaño en m^2: +120
- Número de habitaciones: 3
- Número de ocupantes: 3
- Año de construcción: 41-60
- Régimen legal: Otro

## Gráficos

### Correlaciones

```{r, out.width="100%", fig.align='center'}
#| fig.cap = "Correlaciones entre variables demográficas."
include_graphics("grafs/cor/cor_pob.png")
```

```{r, out.width="100%", fig.align='center'}
#| fig.cap = "Correlaciones entre variables de las viviendas."
include_graphics("grafs/cor/cor_casa.png")
```

```{r, out.width="100%", fig.align='center'}
#| fig.cap = "Correlaciones entre todas las variables."
include_graphics("grafs/cor/cor_df.png")
```

### Puntuaciones factoriales

```{r, out.width="90%"}
for(i in 1:7){
  hist(fit_2$scores[,i], main = paste0(i, ". ", nb_fact[i]), xlab = NULL, ylab = NULL)
}
```


## Influencia factor-variable

```{r, echo = TRUE}
print(fit_2[["loadings"]], digits = 2, cutoff = .1, sort = TRUE)
```



\newpage
# Bibliografía

- [Datos vectoriales](https://www.juntadeandalucia.es/institutodeestadisticaycartografia/dega/datos-espaciales-de-referencia-de-andalucia-dera/descarga-de-informacion)

- [Análisis factorial I](https://www.karlin.mff.cuni.cz/~maciak/NMST539/cvicenie8.html)

- [Análisis factorial II](https://www.geo.fu-berlin.de/en/v/soga-r/Advances-statistics/Multivariate-approaches/Factor-Analysis/A-Simple-Example-of-Factor-Analysis-in-R/index.html)

- [Análisis clúster I](https://spatialanalysis.github.io/workshop-notes/spatial-clustering.html#clustering-analysis-with-other-r-packages) 

- [Análisis clúster II](https://spatialanalysis.github.io/tutorials/#cluster-analysis-in-r)

- [Análisis clúster III](https://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book/clustering-analysis.html)

- [ClustGeo](https://cran.r-project.org/web/packages/ClustGeo/vignettes/intro_ClustGeo.html)

- [ClusterR](https://cran.r-project.org/web/packages/ClusterR/vignettes/the_clusterR_package.html)
